# Bilingual analysis of logical coherence and accuracy in large language model performance on high school math word problems: Russian vs. English

This repository contains the code and data experimental part of the paper "Bilingual analysis of logical coherence and accuracy in large language model performance on high school math word problems: Russian vs. English".

The main dataset is available [here](https://data.mendeley.com/datasets/52vc6v4czj/1) and [here](https://huggingface.co/datasets/lighteval/QazUNTv2).

The paper is available *soon*.



## Overview

Our study evaluates the reasoning capabilities of several large language models (LLMs) when solving high school math word problems in both Russian and English. We analyze logical coherence, answer accuracy, and reasoning errors using the QazUNTv2 dataset and automated metrics such as BERTScore and ROUGE. Our experiments compare models like Gemini-1.5-Pro, Llama 3.1-406B-Instruct, Mistral-8x22B-Instruct-v0.1, ChatGPT-3.5 Turbo, ChatGPT-4, and ChatGPT-4o.



## Authors:

**路** Aigerim Sirgebayeva

**路** Yersaiyn Yergazy (corresp.) 

**路** Assel Jaxylykova 

**路** Alexandr Pak
